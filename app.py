# file: app.py
import streamlit as st
import chromadb
import google.generativeai as genai

# --- PH·∫¶N C·∫§U H√åNH ---
# API Key c·ªßa b·∫°n t·ª´ Google Cloud
GOOGLE_API_KEY = 'AIzaSyBOAgpJI1voNNxeOC6sS7y01EJRXWSK0YU' # !!! THAY API KEY C·ª¶A B·∫†N V√ÄO ƒê√ÇY !!!
# ƒê∆∞·ªùng d·∫´n t·ªõi c∆° s·ªü d·ªØ li·ªáu vector
DB_PATH = 'yhct_chroma_db'
# T√™n c·ªßa b·ªô s∆∞u t·∫≠p trong database
COLLECTION_NAME = 'yhct_collection'

# --- B·∫¢NG GI√Å C√ÅC M√î H√åNH (USD cho m·ªói 1 tri·ªáu token) ---
# Vui l√≤ng ki·ªÉm tra l·∫°i gi√° tr√™n trang c·ªßa Google ƒë·ªÉ c√≥ th√¥ng tin m·ªõi nh·∫•t
MODEL_PRICING = {
    "gemini-1.5-pro-latest": {
        "input": 3.50,
        "output": 10.50
    },
    "gemini-1.5-flash-latest": {
        "input": 0.35,
        "output": 1.05
    }
}

# --- KH·ªûI T·∫†O AI V√Ä DATABASE ---
@st.cache_resource
def load_db():
    """K·∫øt n·ªëi t·ªõi database ch·ªâ m·ªôt l·∫ßn duy nh·∫•t."""
    try:
        client = chromadb.PersistentClient(path=DB_PATH)
        collection = client.get_collection(name=COLLECTION_NAME)
        return collection
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o Database: {e}")
        return None

# --- H√ÄM LOGIC X·ª¨ L√ù C√ÇU H·ªéI ---
def get_ai_response(question, model, collection, model_name):
    """L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ AI v√† th√¥ng tin s·ª≠ d·ª•ng API."""
    # 1. T√¨m ki·∫øm trong DB
    results = collection.query(
        query_texts=[question],
        n_results=3
    )
    
    retrieved_docs = results['documents'][0]
    retrieved_sources = [meta['source'] for meta in results['metadatas'][0]]

    # 2. T·∫°o prompt
    context = "\n\n---\n\n".join(retrieved_docs)
    prompt = f"""D·ª±a v√†o c√°c th√¥ng tin tham kh·∫£o ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c.
    
    Th√¥ng tin tham kh·∫£o:
    {context}
    
    C√¢u h·ªèi: {question}
    """
    
    # 3. G·ªçi Gemini
    response = model.generate_content(prompt)
    
    # 4. Tr√≠ch xu·∫•t th√¥ng tin s·ª≠ d·ª•ng v√† t√≠nh chi ph√≠
    usage_info = None
    try:
        usage = response.usage_metadata
        prompt_tokens = usage.prompt_token_count
        response_tokens = usage.candidates_token_count
        total_tokens = usage.total_token_count
        
        # L·∫•y gi√° t·ª´ b·∫£ng gi√° d·ª±a tr√™n model ƒë√£ ch·ªçn
        price_input = MODEL_PRICING[model_name]["input"]
        price_output = MODEL_PRICING[model_name]["output"]
        
        input_cost = (prompt_tokens / 1_000_000) * price_input
        output_cost = (response_tokens / 1_000_000) * price_output
        total_cost = input_cost + output_cost
        
        usage_info = {
            "model": model_name,
            "prompt_tokens": prompt_tokens,
            "response_tokens": response_tokens,
            "total_tokens": total_tokens,
            "cost_usd": total_cost
        }
    except Exception as e:
        print(f"Kh√¥ng th·ªÉ l·∫•y th√¥ng tin s·ª≠ d·ª•ng: {e}")

    return response.text, set(retrieved_sources), usage_info

# --- GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG STREAMLIT ---
st.set_page_config(page_title="Tr·ª£ l√Ω Y h·ªçc C·ªï truy·ªÅn", page_icon="üåø")
st.title("üåø Tr·ª£ l√Ω Y h·ªçc C·ªï truy·ªÅn")

# Th√™m thanh b√™n (sidebar) ƒë·ªÉ ng∆∞·ªùi d√πng ch·ªçn m√¥ h√¨nh
with st.sidebar:
    st.header("C·∫•u h√¨nh")
    selected_model = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh AI:",
        options=list(MODEL_PRICING.keys()),
        help="Ch·ªçn m√¥ h√¨nh AI ƒë·ªÉ x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n."
    )
    st.caption(f"B·∫°n ƒë√£ ch·ªçn: `{selected_model}`")
    st.info(
        "**Gemini 1.5 Pro:** M·∫°nh nh·∫•t, th√¥ng minh nh·∫•t, chi ph√≠ cao h∆°n.\n\n"
        "**Gemini 1.5 Flash:** Nhanh h∆°n, chi ph√≠ r·∫ª h∆°n r·∫•t nhi·ªÅu, ph√π h·ª£p cho h·∫ßu h·∫øt c√°c t√°c v·ª• tra c·ª©u."
    )

# C·∫•u h√¨nh API key
try:
    genai.configure(api_key=GOOGLE_API_KEY)
    # Kh·ªüi t·∫°o model d·ª±a tr√™n l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi d√πng
    llm_model = genai.GenerativeModel(selected_model)
    collection = load_db()
except Exception as e:
    st.error(f"L·ªói kh·ªüi t·∫°o. Vui l√≤ng ki·ªÉm tra API Key. L·ªói: {e}")
    llm_model = None
    collection = None

# Ch·ªâ hi·ªÉn th·ªã giao di·ªán chat n·∫øu kh·ªüi t·∫°o th√†nh c√¥ng
if llm_model and collection:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_allow_html=True)

    if prompt := st.chat_input("V√≠ d·ª•: B·ªánh Th√°i D∆∞∆°ng l√† g√¨?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner(f"AI ({selected_model}) ƒëang suy nghƒ©..."):
                response_text, sources, usage_info = get_ai_response(prompt, llm_model, collection, selected_model)
                
                source_markdown = "\n\n---\n**Ngu·ªìn tham kh·∫£o:**\n" + "\n".join([f"- `{s}`" for s in sources])
                full_response_text = response_text + source_markdown
                st.markdown(full_response_text)
                
                if usage_info:
                    with st.expander("Xem chi ti·∫øt s·ª≠ d·ª•ng API"):
                        col1, col2, col3, col4 = st.columns(4)
                        col1.metric("Tokens ƒê·∫ßu v√†o", usage_info['prompt_tokens'])
                        col2.metric("Tokens ƒê·∫ßu ra", usage_info['response_tokens'])
                        col3.metric("T·ªïng Tokens", usage_info['total_tokens'])
                        col4.metric("Chi ph√≠ (USD)", f"${usage_info['cost_usd']:.6f}")
                        st.caption(f"M√¥ h√¨nh s·ª≠ d·ª•ng: `{usage_info['model']}`")

        # L∆∞u l·∫°i to√†n b·ªô n·ªôi dung ƒë√£ hi·ªÉn th·ªã v√†o l·ªãch s·ª≠ chat
        if usage_info:
            usage_html = f"""
            <details>
                <summary>Xem chi ti·∫øt s·ª≠ d·ª•ng API</summary>
                <div style="padding: 10px; background-color: #f0f2f6; border-radius: 5px;">
                    <p><b>M√¥ h√¨nh:</b> {usage_info['model']}</p>
                    <p><b>Tokens ƒê·∫ßu v√†o:</b> {usage_info['prompt_tokens']}</p>
                    <p><b>Tokens ƒê·∫ßu ra:</b> {usage_info['response_tokens']}</p>
                    <p><b>T·ªïng Tokens:</b> {usage_info['total_tokens']}</p>
                    <p><b>Chi ph√≠ (USD):</b> ${usage_info['cost_usd']:.6f}</p>
                </div>
            </details>
            """
            full_response_to_save = full_response_text + usage_html
        else:
            full_response_to_save = full_response_text

        st.session_state.messages.append({"role": "assistant", "content": full_response_to_save})
else:
    st.warning("Vui l√≤ng cung c·∫•p API Key h·ª£p l·ªá v√† ƒë·∫£m b·∫£o ƒë√£ l·∫≠p ch·ªâ m·ª•c d·ªØ li·ªáu ƒë·ªÉ b·∫Øt ƒë·∫ßu.")