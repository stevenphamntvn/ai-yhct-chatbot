# file: app.py
# Phi√™n b·∫£n ƒë√£ s·ª≠a l·ªói ƒë·ªÉ ch·∫°y tr√™n Streamlit Community Cloud

# --- PH·∫¶N S·ª¨A L·ªñI QUAN TR·ªåNG ---
# Ba d√≤ng n√†y ph·∫£i n·∫±m ·ªü ngay ƒë·∫ßu file
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# --------------------------------

import streamlit as st
import chromadb
import google.generativeai as genai
import os
import requests
import zipfile
from io import BytesIO

# --- PH·∫¶N C·∫§U H√åNH ---
# API Key c·ªßa b·∫°n t·ª´ Google Cloud
GOOGLE_API_KEY = 'AIzaSyBOAgpJI1voNNxeOC6sS7y01EJRXWSK0YU' # !!! THAY API KEY C·ª¶A B·∫†N V√ÄO ƒê√ÇY !!!
# T√™n m√¥ h√¨nh b·∫°n mu·ªën s·ª≠ d·ª•ng
MODEL_NAME = 'gemini-1.5-pro-latest'

# --- C·∫§U H√åNH M·ªöI CHO TRI·ªÇN KHAI ONLINE ---
# !!! QUAN TR·ªåNG: D√°n ƒë∆∞·ªùng d·∫´n t·∫£i tr·ª±c ti·∫øp file zip c·ªßa b·∫°n v√†o ƒë√¢y
DB_ZIP_URL = "https://drive.google.com/uc?export=download&id=1-2q9AG84492czMsWmhTbQziBDRyvFP0X"
DB_PATH = 'yhct_chroma_db'
COLLECTION_NAME = 'yhct_collection'


# --- B·∫¢NG GI√Å (C·∫≠p nh·∫≠t th√°ng 7/2025 - Vui l√≤ng ki·ªÉm tra l·∫°i gi√° tr√™n trang c·ªßa Google) ---
PRICE_PER_MILLION_INPUT_TOKENS = 3.50  # $3.50
PRICE_PER_MILLION_OUTPUT_TOKENS = 10.50 # $10.50

# --- H√ÄM T·∫¢I V√Ä GI·∫¢I N√âN DATABASE ---
def setup_database():
    """
    Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa database. N·∫øu kh√¥ng c√≥, t·∫£i v·ªÅ v√† gi·∫£i n√©n.
    H√†m n√†y tr·∫£ v·ªÅ True n·∫øu database s·∫µn s√†ng, False n·∫øu c√≥ l·ªói.
    """
    if not os.path.exists(DB_PATH):
        st.info(f"Kh√¥ng t√¨m th·∫•y database '{DB_PATH}'. B·∫Øt ƒë·∫ßu t·∫£i v·ªÅ t·ª´ cloud...")
        st.warning("Qu√° tr√¨nh n√†y ch·ªâ di·ªÖn ra m·ªôt l·∫ßn khi ·ª©ng d·ª•ng kh·ªüi ƒë·ªông v√† c√≥ th·ªÉ m·∫•t v√†i ph√∫t. Vui l√≤ng kh√¥ng ƒë√≥ng ·ª©ng d·ª•ng.")
        
        if not DB_ZIP_URL or DB_ZIP_URL == "YOUR_DIRECT_DOWNLOAD_LINK_TO_THE_DB_ZIP_FILE":
            st.error("L·ªói c·∫•u h√¨nh: Vui l√≤ng cung c·∫•p ƒë∆∞·ªùng d·∫´n t·∫£i tr·ª±c ti·∫øp (DB_ZIP_URL) trong file app.py.")
            return False

        try:
            # T·∫£i file zip
            with st.spinner('ƒêang t·∫£i database (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...'):
                response = requests.get(DB_ZIP_URL, stream=True)
                response.raise_for_status() # B√°o l·ªói n·∫øu t·∫£i th·∫•t b·∫°i
            
            # Gi·∫£i n√©n file zip v√†o th∆∞ m·ª•c hi·ªán t·∫°i
            with st.spinner('ƒêang gi·∫£i n√©n database...'):
                with zipfile.ZipFile(BytesIO(response.content)) as z:
                    z.extractall('.')
            
            st.success("Database ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p th√†nh c√¥ng! ƒêang t·∫£i l·∫°i ·ª©ng d·ª•ng...")
            # Ch·ªù m·ªôt ch√∫t ƒë·ªÉ h·ªá th·ªëng file ·ªïn ƒë·ªãnh r·ªìi ch·∫°y l·∫°i app
            st.rerun()
            
        except Exception as e:
            st.error(f"ƒê√£ x·∫£y ra l·ªói khi t·∫£i ho·∫∑c gi·∫£i n√©n database: {e}")
            return False
            
    return True

# --- KH·ªûI T·∫†O AI V√Ä DATABASE ---
@st.cache_resource
def load_models_and_db():
    """T·∫£i model AI v√† k·∫øt n·ªëi t·ªõi database ch·ªâ m·ªôt l·∫ßn duy nh·∫•t."""
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        llm_model = genai.GenerativeModel(MODEL_NAME)
        
        client = chromadb.PersistentClient(path=DB_PATH)
        collection = client.get_collection(name=COLLECTION_NAME)
        
        return llm_model, collection
    except Exception as e:
        # L·ªói n√†y th∆∞·ªùng x·∫£y ra n·∫øu database ch∆∞a ƒë∆∞·ª£c t·∫£i v·ªÅ xong
        st.error(f"L·ªói kh·ªüi t·∫°o database: {e}. C√≥ th·ªÉ database ƒëang ƒë∆∞·ª£c t·∫£i v·ªÅ. Trang s·∫Ω t·ª± l√†m m·ªõi.")
        return None, None

# --- H√ÄM LOGIC X·ª¨ L√ù C√ÇU H·ªéI ---
def get_ai_response(question, model, collection):
    """L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ AI v√† th√¥ng tin s·ª≠ d·ª•ng API."""
    # 1. T√¨m ki·∫øm trong DB
    results = collection.query(
        query_texts=[question],
        n_results=3
    )
    
    retrieved_docs = results['documents'][0]
    retrieved_sources = [meta['source'] for meta in results['metadatas'][0]]

    # 2. T·∫°o prompt
    context = "\n\n---\n\n".join(retrieved_docs)
    prompt = f"""D·ª±a v√†o c√°c th√¥ng tin tham kh·∫£o ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
    
    Th√¥ng tin tham kh·∫£o:
    {context}
    
    C√¢u h·ªèi: {question}
    """
    
    # 3. G·ªçi Gemini
    response = model.generate_content(prompt)
    
    # 4. Tr√≠ch xu·∫•t th√¥ng tin s·ª≠ d·ª•ng t·ª´ ph·∫£n h·ªìi c·ªßa API
    usage_info = None
    try:
        usage = response.usage_metadata
        prompt_tokens = usage.prompt_token_count
        response_tokens = usage.candidates_token_count
        total_tokens = usage.total_token_count
        
        # T√≠nh to√°n chi ph√≠
        input_cost = (prompt_tokens / 1_000_000) * PRICE_PER_MILLION_INPUT_TOKENS
        output_cost = (response_tokens / 1_000_000) * PRICE_PER_MILLION_OUTPUT_TOKENS
        total_cost = input_cost + output_cost
        
        usage_info = {
            "model": MODEL_NAME,
            "prompt_tokens": prompt_tokens,
            "response_tokens": response_tokens,
            "total_tokens": total_tokens,
            "cost_usd": total_cost
        }
    except Exception as e:
        print(f"Kh√¥ng th·ªÉ l·∫•y th√¥ng tin s·ª≠ d·ª•ng: {e}")

    return response.text, set(retrieved_sources), usage_info

# --- GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG STREAMLIT ---
st.set_page_config(page_title="Tr·ª£ l√Ω Y h·ªçc C·ªï truy·ªÅn", page_icon="üåø")
st.title("üåø Tr·ª£ l√Ω Y h·ªçc C·ªï truy·ªÅn")
st.write("ƒê·∫∑t c√¢u h·ªèi ƒë·ªÉ tra c·ª©u ki·∫øn th·ª©c t·ª´ kho d·ªØ li·ªáu y h·ªçc c·ªï truy·ªÅn.")

# B∆∞·ªõc 1: ƒê·∫£m b·∫£o database ƒë√£ s·∫µn s√†ng
database_ready = setup_database()

# B∆∞·ªõc 2: Ch·ªâ t·∫£i model v√† hi·ªÉn th·ªã giao di·ªán chat n·∫øu database ƒë√£ s·∫µn s√†ng
if database_ready:
    llm_model, collection = load_models_and_db()

    if llm_model and collection:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"], unsafe_allow_html=True)

        if prompt := st.chat_input("V√≠ d·ª•: B·ªánh Th√°i D∆∞∆°ng l√† g√¨?"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("AI ƒëang ph√¢n t√≠ch v√† t·ªïng h·ª£p..."):
                    response_text, sources, usage_info = get_ai_response(prompt, llm_model, collection)
                    
                    # T·∫°o n·ªôi dung hi·ªÉn th·ªã ch√≠nh
                    source_markdown = "\n\n---\n**Ngu·ªìn tham kh·∫£o:**\n" + "\n".join([f"- `{s}`" for s in sources])
                    full_response = response_text + source_markdown
                    st.markdown(full_response)
                    
                    # Hi·ªÉn th·ªã th√¥ng tin s·ª≠ d·ª•ng API trong m·ªôt expander
                    if usage_info:
                        with st.expander("Xem chi ti·∫øt s·ª≠ d·ª•ng API"):
                            col1, col2, col3, col4 = st.columns(4)
                            col1.metric("Tokens ƒê·∫ßu v√†o", usage_info['prompt_tokens'])
                            col2.metric("Tokens ƒê·∫ßu ra", usage_info['response_tokens'])
                            col3.metric("T·ªïng Tokens", usage_info['total_tokens'])
                            col4.metric("Chi ph√≠ (USD)", f"${usage_info['cost_usd']:.6f}")
                            st.caption(f"M√¥ h√¨nh s·ª≠ d·ª•ng: `{usage_info['model']}`")

            # L∆∞u l·∫°i to√†n b·ªô n·ªôi dung ƒë√£ hi·ªÉn th·ªã v√†o l·ªãch s·ª≠ chat
            if usage_info:
                usage_html = f"""
                <details>
                    <summary>Xem chi ti·∫øt s·ª≠ d·ª•ng API</summary>
                    <div style="padding: 10px; background-color: #f0f2f6; border-radius: 5px;">
                        <p><b>M√¥ h√¨nh:</b> {usage_info['model']}</p>
                        <p><b>Tokens ƒê·∫ßu v√†o:</b> {usage_info['prompt_tokens']}</p>
                        <p><b>Tokens ƒê·∫ßu ra:</b> {usage_info['response_tokens']}</p>
                        <p><b>T·ªïng Tokens:</b> {usage_info['total_tokens']}</p>
                        <p><b>Chi ph√≠ (USD):</b> ${usage_info['cost_usd']:.6f}</p>
                    </div>
                </details>
                """
                full_response += usage_html

            st.session_state.messages.append({"role": "assistant", "content": full_response})
