# file: app.py
# Phi√™n b·∫£n ho√†n ch·ªânh: Ch·∫°y online, s·ª≠a l·ªói, v√† cho ph√©p l·ª±a ch·ªçn m√¥ h√¨nh AI.

# --- PH·∫¶N S·ª¨A L·ªñI QUAN TR·ªåNG CHO STREAMLIT CLOUD ---
# Ba d√≤ng n√†y ph·∫£i n·∫±m ·ªü ngay ƒë·∫ßu file
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# ----------------------------------------------------

import streamlit as st
import chromadb
import google.generativeai as genai
import os
import requests
import zipfile
from io import BytesIO

# --- PH·∫¶N C·∫§U H√åNH ---
# API Key c·ªßa b·∫°n t·ª´ Google Cloud
GOOGLE_API_KEY = 'AIzaSyBOAgpJI1voNNxeOC6sS7y01EJRXWSK0YU' # !!! THAY API KEY C·ª¶A B·∫†N V√ÄO ƒê√ÇY !!!

# --- C·∫§U H√åNH TRI·ªÇN KHAI ONLINE ---
# !!! QUAN TR·ªåNG: D√°n ƒë∆∞·ªùng d·∫´n t·∫£i tr·ª±c ti·∫øp file zip c·ªßa b·∫°n v√†o ƒë√¢y
DB_ZIP_URL = "https://drive.google.com/uc?export=download&id=1-2q9AG84492czMsWmhTbQziBDRyvFP0X"
DB_PATH = 'yhct_chroma_db'
COLLECTION_NAME = 'yhct_collection'

# --- B·∫¢NG GI√Å V√Ä L·ª∞A CH·ªåN M√î H√åNH ---
MODEL_PRICING = {
    "gemini-1.5-flash-latest": {
        "input": 0.35,  # $0.35 cho m·ªói 1 tri·ªáu token
        "output": 1.05  # $1.05 cho m·ªói 1 tri·ªáu token
    },
    "gemini-1.5-pro-latest": {
        "input": 3.50,  # $3.50 cho m·ªói 1 tri·ªáu token
        "output": 10.50 # $10.50 cho m·ªói 1 tri·ªáu token
    }
}
MODEL_OPTIONS = list(MODEL_PRICING.keys())

# --- H√ÄM T·∫¢I V√Ä GI·∫¢I N√âN DATABASE ---
def setup_database():
    """Ki·ªÉm tra, t·∫£i v·ªÅ v√† gi·∫£i n√©n database n·∫øu c·∫ßn."""
    if not os.path.exists(DB_PATH):
        st.info(f"Kh√¥ng t√¨m th·∫•y database '{DB_PATH}'. B·∫Øt ƒë·∫ßu t·∫£i v·ªÅ t·ª´ cloud...")
        st.warning("Qu√° tr√¨nh n√†y ch·ªâ di·ªÖn ra m·ªôt l·∫ßn v√† c√≥ th·ªÉ m·∫•t v√†i ph√∫t.")
        
        if not DB_ZIP_URL or DB_ZIP_URL == "YOUR_DIRECT_DOWNLOAD_LINK_TO_THE_DB_ZIP_FILE":
            st.error("L·ªói c·∫•u h√¨nh: Vui l√≤ng cung c·∫•p DB_ZIP_URL trong file app.py.")
            return False

        try:
            with st.spinner('ƒêang t·∫£i database...'):
                response = requests.get(DB_ZIP_URL)
                response.raise_for_status()
            with st.spinner('ƒêang gi·∫£i n√©n...'):
                with zipfile.ZipFile(BytesIO(response.content)) as z:
                    z.extractall('.')
            st.success("Thi·∫øt l·∫≠p database th√†nh c√¥ng! ƒêang t·∫£i l·∫°i...")
            st.rerun()
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i ho·∫∑c gi·∫£i n√©n database: {e}")
            return False
    return True

# --- KH·ªûI T·∫†O DATABASE ---
@st.cache_resource
def load_db():
    """K·∫øt n·ªëi t·ªõi database v√† tr·∫£ v·ªÅ collection."""
    try:
        client = chromadb.PersistentClient(path=DB_PATH)
        collection = client.get_collection(name=COLLECTION_NAME)
        return collection
    except Exception as e:
        st.error(f"L·ªói k·∫øt n·ªëi database: {e}")
        return None

# --- H√ÄM LOGIC X·ª¨ L√ù C√ÇU H·ªéI ---
def get_ai_response(question, model, collection, model_name):
    """L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ AI v√† th√¥ng tin s·ª≠ d·ª•ng."""
    results = collection.query(query_texts=[question], n_results=3)
    context = "\n\n---\n\n".join(results['documents'][0])
    prompt = f"""D·ª±a v√†o th√¥ng tin tham kh·∫£o d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
    Th√¥ng tin tham kh·∫£o: {context}
    C√¢u h·ªèi: {question}"""
    
    response = model.generate_content(prompt)
    usage_info = None
    try:
        usage = response.usage_metadata
        prompt_tokens = usage.prompt_token_count
        response_tokens = usage.candidates_token_count
        
        price_input = MODEL_PRICING[model_name]["input"]
        price_output = MODEL_PRICING[model_name]["output"]
        
        input_cost = (prompt_tokens / 1_000_000) * price_input
        output_cost = (response_tokens / 1_000_000) * price_output
        
        usage_info = {
            "model": model_name,
            "prompt_tokens": prompt_tokens,
            "response_tokens": response_tokens,
            "total_tokens": usage.total_token_count,
            "cost_usd": input_cost + output_cost
        }
    except Exception:
        pass # B·ªè qua n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c th√¥ng tin s·ª≠ d·ª•ng

    return response.text, set(meta['source'] for meta in results['metadatas'][0]), usage_info

# --- GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG STREAMLIT ---
st.set_page_config(page_title="Tr·ª£ l√Ω Y h·ªçc C·ªï truy·ªÅn", page_icon="üåø")
st.title("üåø Tr·ª£ l√Ω Y h·ªçc C·ªï truy·ªÅn")

# Thanh b√™n ƒë·ªÉ ch·ªçn m√¥ h√¨nh
with st.sidebar:
    st.header("C·∫•u h√¨nh")
    selected_model = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh AI:",
        options=MODEL_OPTIONS,
        index=0, # M·∫∑c ƒë·ªãnh ch·ªçn 'gemini-1.5-flash-latest'
        help="Flash nhanh v√† r·∫ª h∆°n, Pro th√¥ng minh h∆°n."
    )
    st.caption(f"B·∫°n ƒë√£ ch·ªçn: `{selected_model}`")

# B∆∞·ªõc 1: ƒê·∫£m b·∫£o database ƒë√£ s·∫µn s√†ng
if setup_database():
    # B∆∞·ªõc 2: Kh·ªüi t·∫°o AI v√† DB
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        llm_model = genai.GenerativeModel(selected_model)
        collection = load_db()
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o AI. Vui l√≤ng ki·ªÉm tra API Key. L·ªói: {e}")
        llm_model = None
        collection = None

    # B∆∞·ªõc 3: Hi·ªÉn th·ªã giao di·ªán chat
    if llm_model and collection:
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"], unsafe_allow_html=True)

        if prompt := st.chat_input("V√≠ d·ª•: B·ªánh Th√°i D∆∞∆°ng l√† g√¨?"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner(f"AI ({selected_model}) ƒëang suy nghƒ©..."):
                    response_text, sources, usage_info = get_ai_response(prompt, llm_model, collection, selected_model)
                    
                    source_markdown = "\n\n---\n**Ngu·ªìn tham kh·∫£o:**\n" + "\n".join([f"- `{s}`" for s in sources])
                    full_response_text = response_text + source_markdown
                    st.markdown(full_response_text)
                    
                    if usage_info:
                        with st.expander("Xem chi ti·∫øt s·ª≠ d·ª•ng API"):
                            st.metric("Chi ph√≠ (USD)", f"${usage_info['cost_usd']:.6f}")
                            st.caption(f"Tokens: {usage_info['total_tokens']} | ƒê·∫ßu v√†o: {usage_info['prompt_tokens']} | ƒê·∫ßu ra: {usage_info['response_tokens']}")

            # L∆∞u l·∫°i v√†o l·ªãch s·ª≠ chat
            if usage_info:
                usage_html = f"""
                <details>
                    <summary>Xem chi ti·∫øt s·ª≠ d·ª•ng API</summary>
                    <p><b>Chi ph√≠ (USD):</b> ${usage_info['cost_usd']:.6f}</p>
                </details>
                """
                full_response_to_save = full_response_text + usage_html
            else:
                full_response_to_save = full_response_text
            st.session_state.messages.append({"role": "assistant", "content": full_response_to_save})
